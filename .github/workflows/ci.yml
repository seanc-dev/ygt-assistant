name: CI

on:
  pull_request:
  push:
    branches: [main, feat/**, chore/**]

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: ankane/pgvector:latest
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: app
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres" 
          --health-interval 10s 
          --health-timeout 5s 
          --health-retries 5
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Run minimal end-user tests (Microsoft + providers)
        env:
          DEV_MODE: "true"
          USE_MOCK_GRAPH: "true"
        run: |
          pytest -q \
            tests/api/test_ms_connections_oauth.py \
            tests/services/test_providers_flags_routing.py \
            tests/services/test_microsoft_email.py \
            tests/services/test_microsoft_calendar.py
  design-system-ui:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"
          cache-dependency-path: web/package-lock.json
      - name: Install web dependencies
        working-directory: web
        run: npm ci
      - name: Run scoped LucidWork design system tests
        if: github.ref != 'refs/heads/main'
        working-directory: web
        run: npm run test -- -t "LucidWork Design System"
      - name: Run full web test suite
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        working-directory: web
        run: npm run test
  agent-evals:
    if: false # disabled (legacy e2e not maintained)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Run agent evals (@e2e)
        env:
          RUN_E2E: true
        run: pytest -q -m e2e
  llm-loop:
    runs-on: ubuntu-latest
    env:
      DEV_MODE: "true"
      USE_MOCK_GRAPH: "true"
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Run llm_testing minimal loop (in-process)
        run: |
          python - << 'PY'
          from llm_testing.backends.inprocess import InProcessBackend
          from llm_testing.evaluation_loop import MinimalEvaluationLoop
          backend = InProcessBackend()
          loop = MinimalEvaluationLoop(backend)
          out = loop.run_smoke()
          print({k: type(v).__name__ for k,v in out.items()})
          PY
      - name: Run mock LLM scenarios (YAML)
        env:
          OFFLINE_EVAL: "true"
          DEV_MODE: "true"
          USE_MOCK_GRAPH: "true"
        run: |
          python -m llm_testing.runner --scenarios \
            llm_testing/scenarios/plan_today_buffers.yaml \
            llm_testing/scenarios/triage_sla.yaml \
            llm_testing/scenarios/reschedule_ranked.yaml \
            llm_testing/scenarios/reconnect_retry.yaml
      - name: Check if assistant chat & LLM ops tests should run
        id: should_run_e2e
        uses: dorny/paths-filter@v2
        with:
          filters: |
            assistant_llm_ops:
              - 'presentation/api/routes/chat.py'
              - 'presentation/api/routes/workroom.py'
              - 'presentation/api/routes/queue.py'
              - 'presentation/api/routes/dev.py'
              - 'services/llm.py'
              - 'core/services/llm*.py'
              - 'core/domain/llm*.py'
              - 'llm_testing/**'
              - 'presentation/api/repos/workroom.py'
              - 'presentation/api/repos/queue.py'
              - 'presentation/api/repos/tasks.py'
              - 'presentation/api/repos/user_settings.py'
      - name: Run assistant chat & LLM ops e2e scenarios
        if: steps.should_run_e2e.outputs.assistant_llm_ops == 'true' || github.ref == 'refs/heads/main'
        env:
          OFFLINE_EVAL: "true"
          DEV_MODE: "true"
          USE_MOCK_GRAPH: "true"
          LLM_TESTING_MODE: "true"
        run: |
          python -m llm_testing.runner --scenarios \
            llm_testing/scenarios/assistant_chat_happy_path.yaml \
            llm_testing/scenarios/assistant_chat_edge_cases.yaml \
            llm_testing/scenarios/assistant_chat_live_inbox.yaml \
            llm_testing/scenarios/llm_ops_task_training_wheels.yaml \
            llm_testing/scenarios/llm_ops_task_autonomous.yaml \
            llm_testing/scenarios/llm_ops_queue_focus.yaml
      # Skip migrations in this workflow; covered elsewhere
